## ROC and confusion matrix


```{r, include=FALSE}
confusionChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  setup_state(sol_code = check_code, stu_code = user_code)

  msg = errorToMessage(expr = {
    ex() %>% check_object("learner") %>% check_equal()
    ex() %>% check_object("task") %>% check_equal()
    ex() %>% check_object("mod") %>% check_equal()
    ex() %>% check_object("pred") %>% check_equal() 
    ex() %>% check_object("conf_matrix") %>% check_equal() 
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}

```

### Study Goals

*Theoretical (T)*

- Get familiar with confusion matrix and ROC
- Know how to evaluate the performance of a (multiclass) classifier

*Practical (P)*

- Learn how to define and calculate a confusion matrix with `mlr`


### Preparation

1.  *(T)* Watch the following video  (sorry, rather low volume...):
    <center>
![](https://www.youtube.com/watch?v=g3w98HnbtEw&list=PLMyWaJl2LoXyhFvMjtbBGs0Pi8khHbKm3&index=5&t=0s){width="75%"}
    </center>
1. *(P)* Read the `mlr` tutorial about [Predicting Outcomes for New Data](https://mlr.mlr-org.com/articles/tutorial/predict.html#classification-confusion-matrix)



### Exercises

#### *(T)* Quiz


```{r ROC-quiz, echo=FALSE}
  question("Which statements are false?",
    answer("If the proportion of positive to negative instances changes, the ROC curve will not     change."),
    answer("The evaluation of binary classifiers is only the misclassification error.", correct = TRUE),
    answer("Several evaluation metrics can be derived from a confusion matrix."),
    answer("ROC curves are sensitive to class distribution.", correct = TRUE),
    answer("The area under the ROC curve is AUC and AUC $\\in (0, 1]$.", correct = TRUE),
    answer("In a ROC curve, the best classifier lies on the top-right corner.", correct = TRUE),
    answer("AUC = 0 means that all negatives are ranked higher than all positives.")
  )
```

#### *(P)* Define a confusion matrix

For this exercise we want to predict the class labels on the iris task with the LDA learner and calculate the confusion matrix with relative frequencies of these predictions. Finally, we want to add the absolute number of observations for each predicted and true class label to the matrix (both absolute and relative).



```{r confusion, exercise=TRUE, exercise.lines=10, exercise.checker=createChecker("conf_matrix")}
learner = "classif.lda"
task = iris.task
mod = train(learner , task)
pred = 
conf_matrix =
```
```{r confusion-hint-1}
# Function predict() can be used.
```

```{r confusion-hint-2}
# A confusion matrix can be obtained by calling calculateConfusionMatrix().
```

```{r confusion-hint-3}
relative = TRUE
```

```{r confusion-hint-4}
sums = TRUE
```

```{r confusion-solution}
learner = "classif.lda"
task = iris.task
mod = train(learner , task)
pred = predict(mod, task)
conf_matrix = calculateConfusionMatrix(pred, relative = TRUE, sums = TRUE)
```
```{r confusion-check}
learner = "classif.lda"
task = iris.task
mod = train(learner , task)
pred = predict(mod, task)
conf_matrix = calculateConfusionMatrix(pred, relative = TRUE, sums = TRUE)
```