## ROC and confusion matrix


```{r, include=FALSE}
confusionChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  setup_state(sol_code = check_code, stu_code = user_code)

  msg = errorToMessage(expr = {
    ex() %>% check_object("learner") %>% check_equal()
    ex() %>% check_object("task") %>% check_equal()
    ex() %>% check_object("mod") %>% check_equal()
    ex() %>% check_object("pred") %>% check_equal() 
    ex() %>% check_object("conf_matrix") %>% check_equal() 
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}

```

### Study Goals

*Theoretical (T)*

- Get familiar with confusion matrix and ROC
- Know how to evaluate the performance of a (multiclass) classifier

*Practical (P)*

- Learn how to define and calculate a confusion matrix with `mlr`


### Preparation

1.  *(T)* Watch the following video  (sorry, rather low volume...):
    <center>
![](https://www.youtube.com/watch?v=g3w98HnbtEw&list=PLMyWaJl2LoXyhFvMjtbBGs0Pi8khHbKm3&index=5&t=0s){width="75%"}
    </center>
1. *(P)* Read the `mlr` tutorial about [Predicting Outcomes for New Data](https://mlr.mlr-org.com/articles/tutorial/predict.html#classification-confusion-matrix)



### Exercises

#### *(T)* Quiz


```{r ROC-quiz, echo=FALSE}
  question("Which statements are false?",
    answer("If the proportion of positive to negative instances changes, the ROC curve will not     change."),
    answer("The evaluation of binary classifiers is only the misclassification error.", correct = TRUE),
    answer("Several evaluation metrics can be derived from a confusion matrix."),
    answer("ROC curves are sensitive to class distribution.", correct = TRUE),
    answer("The area under the ROC curve is AUC and AUC $\\in (0, 1]$.", correct = TRUE),
    answer("In a ROC curve, the best classifier lies on the top-right corner.", correct = TRUE),
    answer("AUC = 0 means that all negatives are ranked higher than all positives.")
  )
```

#### *(P)* Define a confusion matrix

For this exercise we want to predicte the class labels on the iris task (iris.task) with the LDA learner ("classif.lda") and calculate the confusion matrix of these predictions.



```{r confusion, exercise=TRUE, exercise.lines=10, exercise.checker=createChecker("conf_matrix")}
learner = "classif.lda"
task = iris.task
mod = train(learner , task)
pred = 
conf_matrix =
```
```{r confusion-hint-1}
# Function predict() can be useful to get predictions.
```

```{r confusion-hint-2}
# A confusion matrix can be obtained by calling calculateConfusionMatrix().
```


```{r confusion-solution}
learner = "classif.lda"
task = iris.task
mod = train(learner , task)
pred = predict(mod, task)
conf_matrix = calculateConfusionMatrix(pred)
```
```{r confusion-check}
learner = "classif.lda"
task = iris.task
mod = train(learner , task)
pred = predict(mod, task)
conf_matrix = calculateConfusionMatrix(pred)
```